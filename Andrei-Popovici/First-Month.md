# Andrei-Sorin Popovici - First Month at RIST

Here I will document what I learned during my first month at the Romanian Institute of Science and Technology


# First week
http://karpathy.github.io/2015/05/21/rnn-effectiveness/
I started with Recurrent Neural Networks by reading one of Andrej Karpathy's blog posts which showcased the effectiveness and versatility of RNNs. He showed that a simple network of standard RNN cells can generate any type of text: Shakespeare, abstract algebra, C++ code and, probably, anything.

![](http://karpathy.github.io/assets/rnn/latex3.jpeg)


http://cs231n.github.io/neural-networks-case-study/

From one of Karpathy's source codes I discovered this tutorial which explains the cross entropy loss function, it's gradient and why non-linearity is necessary. I also learned how to create a spiral dataset, which might prove useful sometime in the future.

![](http://cs231n.github.io/assets/eg/spiral_linear.png) 
![](http://cs231n.github.io/assets/eg/spiral_net.png) 
> Linear (49% accuracy) vs Non-linear (98% accuracy)

Coming back to RNNs
http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/
http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/

The next thing natural thing to study was how does and RNN cell looks like, how it works and why it works.

De continuat cu LSTM-uri

## Second week

http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/
LSTMs, LSTM experiments, latent space
Turing test(?)

## Third week

During this week my focus was to implement a CNN 
Neuroscience, experimentul cu lumina - legatura cu CNN-uri

## Fourth week

Autoencoder, 2-SAT, programare liniara (inceput)

https://distill.pub/2016/misread-tsne/
